{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respected-seven",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-valve",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "after-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from tensorflow.keras.layers import Input, Layer, Conv2D, MaxPool2D, Flatten, Dense\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "# Other stuff\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import functools\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-balloon",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suited-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for functions\n",
    "base_path = \"C:/repos/\"\n",
    "sys.path.append(base_path)\n",
    "# Path where configs are stored\n",
    "configs_path = base_path + \"twrds_unbiased_anns/configs/\"\n",
    "\n",
    "# Path top store runs\n",
    "runs_path = base_path + \"twrds_unbiased_anns/runs/\"\n",
    "\n",
    "# Path to store run results\n",
    "results_path = base_path + \"twrds_unbiased_anns/runs/results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-institution",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "difficult-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of functions\n",
    "# Sample creation\n",
    "from twrds_unbiased_anns.src.data.samples import create_sample_array, get_sample_data, get_sample_params, convert_sample_to_np_array, gen_from_sample, dataset_from_gen\n",
    "# Evaluation\n",
    "from twrds_unbiased_anns.src.data.eval import load_eval_samples, evaluate_performance, evaluate_performance_class, evaluate_model, store_results\n",
    "# Models\n",
    "from twrds_unbiased_anns.src.tf.models import get_model\n",
    "# Losses\n",
    "from twrds_unbiased_anns.src.tf.losses import get_loss\n",
    "# Optimizers\n",
    "from twrds_unbiased_anns.src.tf.optimizers import get_optimizer\n",
    "# Utils\n",
    "from twrds_unbiased_anns.src.utils import load_config_from_file, mkdir, rmdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-zimbabwe",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continental-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'C:/repos/twrds_unbiased_anns/runs/grad_fair_selection_base'\n",
      "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'C:/repos/twrds_unbiased_anns/runs/grad_fair_selection_base/logs/'\n",
      "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'C:/repos/twrds_unbiased_anns/runs/grad_fair_selection_base/checkpoints/'\n"
     ]
    }
   ],
   "source": [
    "# Set name for this run\n",
    "run_name = \"grad_fair_selection_base\"\n",
    "\n",
    "# Manual Config\n",
    "name = \"fair_grad_lambda_{}_target_weight_{}\"\n",
    "eval_sample_filename = \"eval_500_mean_100_std_20.data\"\n",
    "\n",
    "# Model Config\n",
    "modelname = \"GRAD\" \n",
    "target_loss_weight = 0.001\n",
    "n_attributes = 2\n",
    "\n",
    "# Data Config\n",
    "dataset_size = 400\n",
    "colors = [\"red\"]\n",
    "m_diff = 50\n",
    "std = 3\n",
    "share = 50\n",
    "category = \"color\"\n",
    "\n",
    "# Training Config\n",
    "repeats_per_model = 3\n",
    "batch_size = 16 \n",
    "n_epochs = 50\n",
    "optimizer = \"Adam\"\n",
    "\n",
    "# Set run directory\n",
    "run_dir = runs_path + run_name\n",
    "\n",
    "# Get current date\n",
    "cur_date = datetime.datetime.today()\n",
    "date_str = cur_date.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# Create directory for this run\n",
    "mkdir(run_dir, delete = False)\n",
    "\n",
    "# Create logdir\n",
    "log_base_dir = run_dir + \"/logs/\"\n",
    "mkdir(log_base_dir, delete = False)\n",
    "\n",
    "# Create checkpoint dir\n",
    "ckp_base_dir = run_dir + \"/checkpoints/\"\n",
    "mkdir(ckp_base_dir, delete = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-timer",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alien-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation sample\n",
    "eval_samples = load_eval_samples(base_path + \"twrds_unbiased_anns/data/eval/\" + eval_sample_filename)\n",
    "\n",
    "def check_validation_fairness(model):\n",
    "    # Set colors\n",
    "    colors = [\"red\"]\n",
    "    # Create empty list of all results\n",
    "    eval_results = []\n",
    "    # Create dictionary with model information \n",
    "    row = {\n",
    "        \"modelname\": \"fair_grad\",\n",
    "    } \n",
    "    # Run eval\n",
    "    evaluate_model(model, eval_samples, row, eval_results, colors) \n",
    "    # Make dataframe of results\n",
    "    df = pd.DataFrame(eval_results)\n",
    "    # Check for prediction accuracy\n",
    "    for prediction in df.groupby([\"shape_color\", \"shape_type\"]).prediction.mean():\n",
    "        if prediction > 80 and prediction < 120:\n",
    "            pass\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Custom Metric for combined loss\n",
    "class CombineMetricsAndSave(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self,**kargs):\n",
    "        super(CombineMetricsAndSave,self).__init__(**kargs)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs = {}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = {}):        \n",
    "        # Calculate combined performance\n",
    "        if logs[\"attr_0_output_accuracy\"] < 0.6 and logs[\"attr_1_output_accuracy\"] < 0.6:              \n",
    "            # Check if predictions are in acceptable range\n",
    "            if check_validation_fairness(self.model):\n",
    "                global lambda_val\n",
    "                global repeat\n",
    "                # Save model and inform user\n",
    "                print(\"Found valid model. Saving model ...\")\n",
    "                model_name = \"grad_fair_{}_{}_{}\".format(lambda_val, repeat, epoch)\n",
    "                self.model.save(ckp_base_dir + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-patrick",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "comfortable-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Gradient reversal operation\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -dy\n",
    "    return y, custom_grad\n",
    "\n",
    "# Layer that reverses the gradient\n",
    "class GradReverse(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x)\n",
    "\n",
    "def get_grad_model_alt(n_attributes, attr_loss_weight, target_loss_weight = 1., classification = False):    \n",
    "    # Input\n",
    "    model_input = Input(shape=(360, 360, 3), name=\"input_img\")\n",
    "    # Feature extractor\n",
    "    x = Conv2D(32, (3, 3), padding = \"same\", activation = \"relu\", name = \"conv_1\")(model_input)\n",
    "    x = MaxPool2D(pool_size = (2, 2), name = \"pool_1\")(x)\n",
    "    x = Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\", name = \"conv_2\")(x)\n",
    "    x = MaxPool2D(pool_size = (2, 2), name = \"pool_2\")(x)\n",
    "    x = Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\", name = \"conv_3\")(x)\n",
    "    x = Flatten(name = \"flat_1\")(x)\n",
    "    # Target branch\n",
    "    x_target = Dense(100, activation=\"relu\", name = \"target_dense_1\")(x)\n",
    "    x_target = Dense(20, activation=\"relu\", name = \"target_dense_2\")(x_target)\n",
    "    if classification:\n",
    "        target_output = Dense(1, activation = \"sigmoid\", name = \"target_output\")(x_target)\n",
    "    else:\n",
    "        target_output = Dense(1, name = \"target_output\")(x_target)\n",
    "    model_outputs = [target_output]    \n",
    "    # Create losses, weights and metrics\n",
    "    if classification:\n",
    "        losses = {\"target_output\": tf.keras.losses.BinaryCrossentropy()}\n",
    "        target_metric = \"accuracy\"\n",
    "    else: \n",
    "        losses = {\"target_output\": tf.keras.losses.MeanSquaredError()}\n",
    "        target_metric = \"mean_squared_error\"\n",
    "    weights = {\"target_output\": target_loss_weight}\n",
    "    metrics = {\"target_output\": target_metric}    \n",
    "    # Split attribute loss over branches\n",
    "    attr_branch_weight = attr_loss_weight / n_attributes    \n",
    "    # Attribute branches\n",
    "    for i in range(n_attributes):\n",
    "        # Gradient reversal layer\n",
    "        x_attr = GradReverse()(x)\n",
    "        # Funnel into sigmoid for binary classification of attributes\n",
    "        x_attr = Dense(100, activation=\"relu\", name = \"attr_{}_dense_1\".format(i))(x)\n",
    "        x_attr = Dense(20, activation=\"relu\", name = \"attr_{}_dense_2\".format(i))(x_attr)\n",
    "        x_attr = GradReverse()(x_attr)\n",
    "        output_name = \"attr_{}_output\".format(i)\n",
    "        attr_output = Dense(1, activation = \"sigmoid\", name = output_name)(x_attr) \n",
    "        # Add to outputs\n",
    "        model_outputs.append(attr_output)\n",
    "        # Add to losses, weights and metrics\n",
    "        losses[output_name] = tf.keras.losses.BinaryCrossentropy()\n",
    "        weights[output_name] = attr_branch_weight\n",
    "        metrics[output_name] = \"accuracy\"        \n",
    "    # Make model\n",
    "    model = tf.keras.Model(inputs = model_input, outputs = model_outputs, name = \"GRAD_CNN\")       \n",
    "    # Compile model\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(), loss = losses, loss_weights = weights, metrics = metrics)    \n",
    "    # Return finished model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-mississippi",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-grenada",
   "metadata": {},
   "source": [
    "## TensorBoard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "independent-theory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9220), started 0:01:29 ago. (Use '!kill 9220' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e7b91db439add1d7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e7b91db439add1d7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete tensorboard temp dir\n",
    "#rmdir(\"C:/Users/lucas/AppData/Local/Temp/.tensorboard-info\")\n",
    "# Load Tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=$log_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polished-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list of all results\n",
    "results = []\n",
    "\n",
    "# Load evaluation sample\n",
    "eval_samples = load_eval_samples(base_path + \"twrds_unbiased_anns/data/eval/\" + eval_sample_filename)\n",
    "\n",
    "# Calculate number of steps per epoch\n",
    "n_steps = int(dataset_size/batch_size)\n",
    "\n",
    "# Clear session once and then every time before a new model is trained\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Get sample parameters\n",
    "white_square, white_circle, colorful_square, colorful_circle = get_sample_params(category, m_diff, std, share)\n",
    "\n",
    "# Prepare and save sample\n",
    "train_sample = create_sample_array(dataset_size, white_square, white_circle, colorful_square, colorful_circle)\n",
    "cur_name = name.format(1, target_loss_weight)\n",
    "sample_filename = run_dir + \"/\" + \"sample_{}_{}\".format(cur_name, date_str)\n",
    "np.save(file = sample_filename, arr = train_sample)\n",
    "\n",
    "# Create dataset from training data sample\n",
    "data = dataset_from_gen(train_sample, n_epochs, batch_size, colors, attributes = [\"color\", \"shape\"]) \n",
    "\n",
    "# Loop through different lambda values\n",
    "for lambda_val in [10,100,1000]:\n",
    "    \n",
    "    cur_name = name.format(lambda_val, target_loss_weight)\n",
    "\n",
    "    # Loop training for number of repeats\n",
    "    for repeat in range(1, repeats_per_model + 1):   \n",
    "\n",
    "        # Clear keras session\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # Create model (no compilation needed as GRAD is already compiled)\n",
    "        model = get_model(modelname, task_type = \"reg\", target_loss_weight = target_loss_weight, attr_loss_weight = lambda_val, n_attributes = n_attributes)\n",
    "        #model = get_grad_model_alt(n_attributes = n_attributes, attr_loss_weight = lambda_val, target_loss_weight = target_loss_weight, classification = False)\n",
    "        \n",
    "        # Create logdir and callback\n",
    "        logdir = log_base_dir + cur_name + \"_\" + str(repeat)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logdir)\n",
    "\n",
    "        # Do training\n",
    "        model.fit(data, epochs = n_epochs, steps_per_epoch = n_steps, verbose = 0, callbacks=[CombineMetricsAndSave(), tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
