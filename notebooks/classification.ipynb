{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respected-seven",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-valve",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "after-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "# Other stuff\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import functools\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-gibraltar",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increasing-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for functions\n",
    "base_path = \"C:/repos/\"\n",
    "sys.path.append(base_path)\n",
    "\n",
    "# Path where configs are stored\n",
    "configs_path = base_path + \"twrds_unbiased_anns/configs/\"\n",
    "\n",
    "# Path top store runs\n",
    "runs_path = base_path + \"twrds_unbiased_anns/runs/\"\n",
    "\n",
    "# Path to store run results\n",
    "results_path = base_path + \"twrds_unbiased_anns/runs/results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-yacht",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "desperate-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of functions\n",
    "# Sample creation\n",
    "from twrds_unbiased_anns.src.data.samples import create_sample_array, get_sample_data, get_sample_params, convert_sample_to_np_array, gen_from_sample, dataset_from_gen\n",
    "# Evaluation\n",
    "from twrds_unbiased_anns.src.data.eval import load_eval_samples, evaluate_performance, evaluate_performance_class, evaluate_model, store_results\n",
    "# Models\n",
    "from twrds_unbiased_anns.src.tf.models import get_model\n",
    "# Losses\n",
    "from twrds_unbiased_anns.src.tf.losses import get_loss\n",
    "# Optimizers\n",
    "from twrds_unbiased_anns.src.tf.optimizers import get_optimizer\n",
    "# Utils\n",
    "from twrds_unbiased_anns.src.utils import load_config_from_file, mkdir, rmdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-upset",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "backed-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'C:/repos/twrds_unbiased_anns/runs/classification_all'\n",
      "Deleting directory and creating again ...\n"
     ]
    }
   ],
   "source": [
    "# Set name for this run\n",
    "run_name = \"classification_all\" # Also name of the config file\n",
    "\n",
    "# Set run directory\n",
    "run_dir = runs_path + run_name\n",
    "\n",
    "# Get current date\n",
    "cur_date = datetime.datetime.today()\n",
    "date_str = cur_date.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# Load all variables from config file\n",
    "config_filename = run_name + \".json\"\n",
    "name, eval_sample_filename, dataset_size, colors, optimizer, repeats_per_model, batch_size, n_epochs, mean_diffs, stddevs, minority_shares, categorical, models, loss_functions, thresholds, noises = load_config_from_file(configs_path + config_filename, \"class\")\n",
    "\n",
    "# Create directory for this run\n",
    "mkdir(run_dir)\n",
    "\n",
    "# Create logdir\n",
    "log_base_dir = run_dir + \"/logs/\"\n",
    "mkdir(log_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-program",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-julian",
   "metadata": {},
   "source": [
    "## TensorBoard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "usual-investigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8768), started 0:02:06 ago. (Use '!kill 8768' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-93a829c51a697285\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-93a829c51a697285\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete tensorboard temp dir\n",
    "#rmdir(\"C:/Users/lucas/AppData/Local/Temp/.tensorboard-info\")\n",
    "# Load Tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list of all results\n",
    "results = []\n",
    "\n",
    "# Load evaluation sample\n",
    "eval_samples = load_eval_samples(base_path + \"twrds_unbiased_anns/data/eval/\" + eval_sample_filename)\n",
    "\n",
    "# Calculate number of steps per epoch\n",
    "n_steps = int(dataset_size/batch_size)\n",
    "\n",
    "# Iterate over all variable parameter combinations\n",
    "for (modelname, lossname, category, m_diff, std, share, threshold, noise) in itertools.product(models, loss_functions, categorical, mean_diffs, stddevs, minority_shares, thresholds, noises):\n",
    "\n",
    "  # Get name of current iteration\n",
    "  cur_name = name.format(modelname, lossname, category, m_diff, std, share, threshold, noise)\n",
    "\n",
    "  # Clear session once and then every time before a new model is trained\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "  # Get sample parameters\n",
    "  white_square, white_circle, colorful_square, colorful_circle = get_sample_params(category, m_diff, std, share)\n",
    "\n",
    "  # Prepare and save sample\n",
    "  train_sample = create_sample_array(dataset_size, white_square, white_circle, colorful_square, colorful_circle)\n",
    "  sample_filename = run_dir + \"/\" + \"sample_{}_{}\".format(cur_name, date_str)\n",
    "  np.save(file = sample_filename, arr = train_sample)\n",
    "\n",
    "  # Create dataset from training data sample\n",
    "  data = dataset_from_gen(train_sample, n_epochs, batch_size, colors, task_type = \"class\", threshold = threshold, noise = noise, distractor = category) \n",
    "\n",
    "  # Loop training for number of repeats\n",
    "  for repeat in range(1, repeats_per_model + 1):   \n",
    "\n",
    "    # Clear keras session\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Create model and compile it\n",
    "    model = get_model(modelname, task_type = \"class\")\n",
    "    model.compile(optimizer = get_optimizer(optimizer), loss = get_loss(lossname))  \n",
    "\n",
    "    # Create logdir and callback\n",
    "    logdir = log_base_dir + cur_name + \"_\" + str(repeat)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logdir)\n",
    "\n",
    "    # Do training\n",
    "    model.fit(data, epochs = n_epochs, steps_per_epoch = n_steps, verbose = 0, callbacks=[tensorboard_callback])\n",
    "\n",
    "    # Evaluate model\n",
    "    # Create dictionary with model information\n",
    "    row = {\n",
    "        \"run\": run_name,\n",
    "        \"date\": cur_date,\n",
    "        \"model\": modelname,\n",
    "        \"loss\": lossname,\n",
    "        \"category\": category,\n",
    "        \"m_diff\": m_diff,\n",
    "        \"stddev\": std,\n",
    "        \"minority_share\": share,\n",
    "        \"repeat\": repeat,\n",
    "        \"threshold\": threshold,\n",
    "        \"noise\": noise\n",
    "    } \n",
    "    # Run eval\n",
    "    evaluate_model(model, eval_samples, row, results, colors, task_type = \"class\", threshold = 75)   \n",
    "\n",
    "# Store total results as excel\n",
    "excel_name = \"{}_{}_results.xlsx\".format(run_name, date_str)\n",
    "filepath = results_path + excel_name\n",
    "store_results(results, filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
