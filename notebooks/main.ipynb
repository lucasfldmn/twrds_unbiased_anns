{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von img_gen_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8aQe2dIl8D1ILgq5vN+tk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasfldmn/twrds_unbiased_anns/blob/main/notebooks/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX77vHqrfkan"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3lNTU7Rfn63"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBFiUWN1uzQ3"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import datetime\n",
        "import pickle\n",
        "import functools\n",
        "import json\n",
        "import datetime\n",
        "import gc\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlEYbMVFfodx"
      },
      "source": [
        "## GitHub\r\n",
        "\r\n",
        "First we clone the existing repository in order to access the function created in the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-EPP3tnfkSz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "5d358115-456f-43f4-ba76-0020cbd5dde3"
      },
      "source": [
        "from getpass import getpass\r\n",
        "git_user = \"lucasfldmn\"\r\n",
        "git_mail = \"lucfeldmann23@googlemail.com\"\r\n",
        "git_password = getpass('Git Password:')\r\n",
        "\r\n",
        "!git config --global user.email '$git_mail'\r\n",
        "!git config --global user.name '$git_user'\r\n",
        "\r\n",
        "!git clone https://$git_user:$git_password\\@github.com/lucasfldmn/twrds_unbiased_anns/ -q"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b6b4d59ec5f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgit_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lucasfldmn\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgit_mail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lucfeldmann23@googlemail.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgit_password\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Git Password:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"git config --global user.email '$git_mail'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEsd5aZZu3P-"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWVj-xXZu-9P"
      },
      "source": [
        "## Functions for Drawing\r\n",
        "\r\n",
        "These functions are implemented in the repo and can easily be imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXgN3feFf-Kg"
      },
      "source": [
        "from twrds_unbiased_anns.src.data.shapes import make_square, make_circle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJn9L4FHhC4g"
      },
      "source": [
        "## Creation of Sample Array\r\n",
        "\r\n",
        "Now we create a numpy array of the sample that contains all information. This we can store and load in order to reproduce sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNGkSNFZu0du"
      },
      "source": [
        "def draw_from_truncated_normal_distribution(n_samples, mean, stddev = 10):\n",
        "  # Set lower and upper bounds for truncation\n",
        "  lower = 1\n",
        "  upper = 100\n",
        "  # Set parameters of normal distribution\n",
        "  mu = mean\n",
        "  sigma = stddev\n",
        "  # Randomly sample\n",
        "  samples = stats.truncnorm.rvs((lower-mu)/sigma, (upper-mu)/sigma, loc = mu, scale = sigma, size = n_samples)\n",
        "  return np.reshape(samples.round(), (n_samples, 1))\n",
        "\n",
        "def create_sample_array(n_samples, white_square, white_circle, colorful_square, colorful_circle):\n",
        "  # Calculate number of samples for each group\n",
        "  n_white_square = round(n_samples * white_square[0] / 100)\n",
        "  n_white_circle = round(n_samples * white_circle[0] / 100)\n",
        "  n_colorful_square = round(n_samples * colorful_square[0] / 100)\n",
        "  n_colorful_circle = round(n_samples * colorful_circle[0] / 100)\n",
        "\n",
        "  # White squares\n",
        "  color = np.ones((n_white_square,1), dtype = bool) # True = white\n",
        "  shape = np.ones((n_white_square,1), dtype = bool) # True = square\n",
        "  size = draw_from_truncated_normal_distribution(n_white_square, mean = white_square[1], stddev = white_square[2])\n",
        "  white_squares = np.hstack((color, shape, size))\n",
        "\n",
        "  # White circles\n",
        "  color = np.ones((n_white_circle,1), dtype = bool) # True = white\n",
        "  shape = np.zeros((n_white_circle,1), dtype = bool) # False = circle\n",
        "  size = draw_from_truncated_normal_distribution(n_white_circle, mean = white_circle[1], stddev = white_circle[2])\n",
        "  white_circles = np.hstack((color, shape, size))\n",
        "\n",
        "  # Colorful squares\n",
        "  color = np.zeros((n_colorful_square,1), dtype = bool) # False = colorful\n",
        "  shape = np.ones((n_colorful_square,1), dtype = bool) # True = square\n",
        "  size = draw_from_truncated_normal_distribution(n_colorful_square, mean = colorful_square[1], stddev = colorful_square[2])\n",
        "  colorful_squares = np.hstack((color, shape, size))\n",
        "\n",
        "  # Colorful circles\n",
        "  color = np.zeros((n_colorful_circle,1), dtype = bool) # False = colorful\n",
        "  shape = np.zeros((n_colorful_circle,1), dtype = bool) # False = circle\n",
        "  size = draw_from_truncated_normal_distribution(n_colorful_circle, mean = colorful_circle[1], stddev = colorful_circle[2])\n",
        "  colorful_circles = np.hstack((color, shape, size))\n",
        "\n",
        "  # Stack all together\n",
        "  samples = np.vstack((white_squares, white_circles, colorful_squares, colorful_circles))\n",
        "\n",
        "  # Shuffle array\n",
        "  np.random.shuffle(samples)\n",
        "\n",
        "  # Return result\n",
        "  return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW8bVFANhiSj"
      },
      "source": [
        "## Creation of TensorFlow Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Sv517GgdJU"
      },
      "source": [
        "import imageio\r\n",
        "\r\n",
        "def convert_sample_to_np_array(sample):  \r\n",
        "  # Get sample color\r\n",
        "  if sample[0]:\r\n",
        "    sample_color = 'white'\r\n",
        "  else:\r\n",
        "    sample_color = np.random.choice(colors)\r\n",
        "\r\n",
        "  # Get size of sample\r\n",
        "  sample_size = sample[2]\r\n",
        "\r\n",
        "  # Call shape generator based on sample shape\r\n",
        "  if sample[1]:\r\n",
        "    shape = make_square(color = sample_color, size = sample_size)\r\n",
        "  else:\r\n",
        "    shape = make_circle(color = sample_color, size = sample_size)\r\n",
        "\r\n",
        "  # Open image as numpy array\r\n",
        "  shape_array = imageio.imread(shape)\r\n",
        "\r\n",
        "  # Return numpy array and size\r\n",
        "  return shape_array, sample_size\r\n",
        "\r\n",
        "def convert_sample_to_tensor(sample):\r\n",
        "  # Convert sample to numpy array\r\n",
        "  sample_np_array, sample_size = convert_sample_to_np_array(sample)\r\n",
        "  # Convert array to tensor\r\n",
        "  img_tensor = tf.convert_to_tensor(sample_np_array, dtype=tf.int32)\r\n",
        "  # Divide image tensor by 255 to normalize values\r\n",
        "  img_tensor = img_tensor / 255\r\n",
        "  # Return tensor and size\r\n",
        "  return img_tensor, sample_size\r\n",
        "\r\n",
        "def make_tf_dataset(samples, divide_by_255 = True):\r\n",
        "  \r\n",
        "  # Iterate over samples and create list of numpy image arrays and list of target\r\n",
        "  images = []\r\n",
        "  targets = []\r\n",
        "  for sample in samples:\r\n",
        "    # Convert sample to numpy array of the image\r\n",
        "    shape_tensor, sample_size = convert_sample_to_np_array(sample)\r\n",
        "    images.append(shape_tensor)\r\n",
        "    targets.append(sample_size)\r\n",
        "\r\n",
        "  # Convert both to tensors\r\n",
        "  img_tensor = tf.convert_to_tensor(images, dtype=tf.int32)\r\n",
        "  target_tensor = tf.convert_to_tensor(targets, dtype=tf.float32)\r\n",
        "\r\n",
        "  # Divide image tensor by 255 to normalize values\r\n",
        "  img_tensor = img_tensor / 255\r\n",
        "  \r\n",
        "  # Convert to tensorflow dataset and return results\r\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((img_tensor, target_tensor))\r\n",
        "  return dataset\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKibAMLxiD2p"
      },
      "source": [
        "# CNN Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3nSluTEm1YL"
      },
      "source": [
        "## Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_s7aGTbl3Om"
      },
      "source": [
        "from tensorflow.keras.layers import Layer, Conv2D, MaxPool2D, Flatten, Dense\r\n",
        "\r\n",
        "class SimpleCNN(tf.keras.Model):\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        super(SimpleCNN, self).__init__()\r\n",
        "        # Define layers\r\n",
        "        self.network_layers = [\r\n",
        "          # Convolutional layers with zero padding and maxpooling inbetween\r\n",
        "          Conv2D(32, (3, 3), padding = \"same\", activation = \"relu\", input_shape=(360, 360, 4)), \r\n",
        "          MaxPool2D(pool_size = (2, 2)),          \r\n",
        "          Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\"),\r\n",
        "          MaxPool2D(pool_size = (2, 2)),\r\n",
        "          Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\"),\r\n",
        "          # Flatten the feature maps\r\n",
        "          Flatten(),\r\n",
        "          # Fully connected layers to funnel flat tensor into single value\r\n",
        "          Dense(100, activation='relu'),\r\n",
        "          Dense(20, activation='relu'),\r\n",
        "          Dense(1)\r\n",
        "        ]    \r\n",
        "        \r\n",
        "    def call(self, x):\r\n",
        "        # Calculate forward step through all layers\r\n",
        "        for layer in self.network_layers:\r\n",
        "          x = layer(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cjPIfxlm3Kh"
      },
      "source": [
        "## ResNet\r\n",
        "\r\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzZnYtk8m9KX"
      },
      "source": [
        "## SENet (Squeeze Excitation)\r\n",
        "\r\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFIyxR9smton"
      },
      "source": [
        "# Model Evaluation\r\n",
        "\r\n",
        "After training the model on different samples, we want to evaluate the performance of the model based on another dataset that contains now bias at all. We then check the average per group combination to see if there is any bias in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfXibq6ieJdZ"
      },
      "source": [
        "## Setup of Evaluation Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIDoYkq3dn3i"
      },
      "source": [
        "def create_eval_samples(n_samples):\r\n",
        "  # Percentage of samples for each group\r\n",
        "  perc_white_square = 25\r\n",
        "  perc_white_circle = 25\r\n",
        "  perc_colorful_square = 25\r\n",
        "  perc_colorful_circle = 25\r\n",
        "\r\n",
        "  # Means of normal distribution for the four groups\r\n",
        "  mean_white_square = 50\r\n",
        "  mean_white_circle = 50\r\n",
        "  mean_colorful_square = 50\r\n",
        "  mean_colorful_circle = 50  \r\n",
        "\r\n",
        "  # Calculate number of samples for each group\r\n",
        "  n_white_square = round(n_samples * perc_white_square / 100)\r\n",
        "  n_white_circle = round(n_samples * perc_white_circle / 100)\r\n",
        "  n_colorful_square = round(n_samples * perc_colorful_square / 100)\r\n",
        "  n_colorful_circle = round(n_samples * perc_colorful_circle / 100)\r\n",
        "\r\n",
        "  # White squares\r\n",
        "  color = np.ones((n_white_square,1), dtype = bool) # True = white\r\n",
        "  shape = np.ones((n_white_square,1), dtype = bool) # True = square\r\n",
        "  size = draw_from_truncated_normal_distribution(n_white_square, mean_white_square)\r\n",
        "  white_squares = np.hstack((color, shape, size))\r\n",
        "\r\n",
        "  # White circles\r\n",
        "  color = np.ones((n_white_circle,1), dtype = bool) # True = white\r\n",
        "  shape = np.zeros((n_white_circle,1), dtype = bool) # False = circle\r\n",
        "  size = draw_from_truncated_normal_distribution(n_white_circle, mean_white_circle)\r\n",
        "  white_circles = np.hstack((color, shape, size))\r\n",
        "\r\n",
        "  # Colorful squares\r\n",
        "  color = np.zeros((n_colorful_square,1), dtype = bool) # False = colorful\r\n",
        "  shape = np.ones((n_colorful_square,1), dtype = bool) # True = square\r\n",
        "  size = draw_from_truncated_normal_distribution(n_colorful_square, mean_colorful_square)\r\n",
        "  colorful_squares = np.hstack((color, shape, size))\r\n",
        "\r\n",
        "  # Colorful circles\r\n",
        "  color = np.zeros((n_colorful_circle,1), dtype = bool) # False = colorful\r\n",
        "  shape = np.zeros((n_colorful_circle,1), dtype = bool) # False = circle\r\n",
        "  size = draw_from_truncated_normal_distribution(n_colorful_circle, mean_colorful_circle)\r\n",
        "  colorful_circles = np.hstack((color, shape, size))\r\n",
        "\r\n",
        "  # Create labeled list of groups \r\n",
        "  samples = list(zip([white_squares, white_circles, colorful_squares, colorful_circles], [\"white_square\", \"white_circle\", \"colorful_square\", \"colorful_circle\"]))\r\n",
        "  return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w3JlTp2dn-o"
      },
      "source": [
        "## Perform Evaluation\r\n",
        "\r\n",
        "For each group, we calculate the total loss, average loss and average prediction after feeding all images in the sample to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqHhBc1TdoS0"
      },
      "source": [
        "def evaluate_performance(group_sample, model):\r\n",
        "  # Feed sample to model and store targets and prediction\r\n",
        "  actual = []\r\n",
        "  prediction = []\r\n",
        "  for single_sample in group_sample:\r\n",
        "    # Convert to tensor\r\n",
        "    shape_tensor, target_size = convert_sample_to_tensor(single_sample)\r\n",
        "    # Reshape the tensor\r\n",
        "    shape_tensor = tf.reshape(shape_tensor, [1,360,360,4])\r\n",
        "    # Feed to model\r\n",
        "    output = model(shape_tensor)\r\n",
        "    # Store prediction and target\r\n",
        "    actual.append(target_size)\r\n",
        "    prediction.append(output)\r\n",
        "\r\n",
        "  # Calculate performance metrics based on target and prediction\r\n",
        "  actual = np.array(actual)\r\n",
        "  prediction = np.array(prediction)\r\n",
        "  diff = actual - prediction\r\n",
        "  # Squared total distance\r\n",
        "  total_squared_distance = np.sum(np.square(diff))\r\n",
        "  # Average distance\r\n",
        "  avg_distance = np.mean(diff)\r\n",
        "  # Average prediction\r\n",
        "  avg_prediction = np.mean(prediction)\r\n",
        "  # Average actual \r\n",
        "  avg_actual = np.mean(actual)\r\n",
        "  return total_squared_distance, avg_distance, avg_prediction, avg_actual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h06KHo3pK-P"
      },
      "source": [
        "# Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQb8qTvxpQhm"
      },
      "source": [
        "## Comparison Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3cZmizgVi-r"
      },
      "source": [
        "def load_configs_from_file(filepath):\r\n",
        "  with open(filepath, 'r') as filehandle:\r\n",
        "    config_json = json.load(filehandle)\r\n",
        "  eval_sample_filename = config_json[\"general\"][\"eval_sample_filename\"]\r\n",
        "  n_eval_samples = config_json[\"general\"][\"eval_sample_size\"]\r\n",
        "  repeats_per_model = config_json[\"general\"][\"repeats_per_model\"]\r\n",
        "  configs = config_json[\"configs\"]\r\n",
        "  return configs, eval_sample_filename, n_eval_samples, repeats_per_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzgY5sTxz7V"
      },
      "source": [
        "# Set name for this run\r\n",
        "run_name = \"std_dev_check\" # Also name of the config file\r\n",
        "\r\n",
        "# Get current date\r\n",
        "date_str = datetime.datetime.today().strftime(\"%d-%m-%Y\")\r\n",
        "\r\n",
        "# Set run directory\r\n",
        "run_dir = \"/content/twrds_unbiased_anns/runs/\" + run_name + \"_\" + date_str\r\n",
        "\r\n",
        "# Load config from JSON file\r\n",
        "config_filename = run_name + \".json\"\r\n",
        "configs, eval_sample_filename, n_eval_samples, repeats_per_model = load_configs_from_file(\"/content/twrds_unbiased_anns/configs/\" + config_filename)\r\n",
        "eval_sample_filename = eval_sample_filename.format(date_str) # Update filename to include current date\r\n",
        "\r\n",
        "# Create directory for this run\r\n",
        "!rm -rf $run_dir\r\n",
        "!mkdir $run_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qZEXDTfpdNW"
      },
      "source": [
        "## Training + Eval Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ij4u1PupgRP"
      },
      "source": [
        "# Clear session once and then every time before a new model is trained\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "# Create empty list of all results\r\n",
        "results = []\r\n",
        "\r\n",
        "# Create evaluation sample and save it -> Will be used for all configurations\r\n",
        "eval_samples = create_eval_samples(n_eval_samples)\r\n",
        "with open(run_dir + \"/\" + eval_sample_filename, 'wb') as filehandle:\r\n",
        "    pickle.dump(eval_samples, filehandle)\r\n",
        "\r\n",
        "# TensorBoard setup\r\n",
        "%load_ext tensorboard\r\n",
        "!rm -rf ./logs/ \r\n",
        "%tensorboard --logdir $run_dir/logs/gradient_tape/\r\n",
        "\r\n",
        "# Loop over all configurations\r\n",
        "for config in configs:\r\n",
        "\r\n",
        "  # Get name of current config\r\n",
        "  config_name = config[\"name\"]\r\n",
        "\r\n",
        "  # Prepare and save sample\r\n",
        "  dataset_size = config[\"dataset_size\"]\r\n",
        "  white_square = [config[\"perc_white_square\"], config[\"mean_white_square\"], config[\"stddev_white_square\"]]\r\n",
        "  white_circle = [config[\"perc_white_circle\"], config[\"mean_white_circle\"], config[\"stddev_white_circle\"]]\r\n",
        "  colorful_square = [config[\"perc_colorful_square\"], config[\"mean_colorful_square\"], config[\"stddev_colorful_square\"]]\r\n",
        "  colorful_circle = [config[\"perc_colorful_circle\"], config[\"mean_colorful_circle\"], config[\"stddev_colorful_circle\"]]\r\n",
        "  train_sample = create_sample_array(config[\"dataset_size\"], white_square, white_circle, colorful_square, colorful_circle)\r\n",
        "  colors = config[\"colors\"]\r\n",
        "  sample_filename = run_dir + \"/\" + \"sample_{}_{}\".format(config_name, date_str)\r\n",
        "  np.save(file = sample_filename, arr = train_sample)\r\n",
        "\r\n",
        "  # Loop training for number of repeats\r\n",
        "  for repeat in range(1, repeats_per_model + 1):\r\n",
        "\r\n",
        "    # Clear keras session\r\n",
        "    tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "    # Set random seed\r\n",
        "    tf.random.set_seed(np.random.randint(1000,9999))\r\n",
        "\r\n",
        "    # Create model  \r\n",
        "    model = SimpleCNN() # TODO select model based on text value in config (for now only simple CNN) \r\n",
        "    model.compile(optimizer = config[\"optimizer\"], loss = config[\"loss_function\"])  \r\n",
        "\r\n",
        "    # Initialize loss function based on parameter (TODO, for now simple mse)\r\n",
        "    loss_function = tf.keras.losses.MeanSquaredError()\r\n",
        "\r\n",
        "    # Initialize optimizer based on parameter (TODO, for now simple Adam)\r\n",
        "    optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "    # Prepare dataset for training\r\n",
        "    train_dataset = make_tf_dataset(train_sample)\r\n",
        "    train_dataset = train_dataset.shuffle(buffer_size = dataset_size)\r\n",
        "    train_dataset = train_dataset.batch(config[\"batch_size\"])\r\n",
        "\r\n",
        "    # Set up tensorboard summary writer\r\n",
        "    model_name = \"model_{}_{}_{}\".format(config_name, repeat, date_str)\r\n",
        "    train_summary_writer = tf.summary.create_file_writer(run_dir + '/logs/gradient_tape/' + model_name + '/train')\r\n",
        "\r\n",
        "    # Do training\r\n",
        "    for epoch in range(config[\"n_epochs\"]):    \r\n",
        "      for (img, target) in train_dataset:\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "          # Forward step\r\n",
        "          output = model(img)\r\n",
        "          # Compute loss\r\n",
        "          loss = loss_function(target, output)\r\n",
        "          # Compute gradient\r\n",
        "          gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "        # Apply gradients\r\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "        # Store loss\r\n",
        "        with train_summary_writer.as_default():\r\n",
        "            tf.summary.scalar('loss', loss, step=epoch)\r\n",
        "\r\n",
        "    # Save trained model\r\n",
        "    mode_filename = run_dir + \"/\" + model_name\r\n",
        "    model.save(filepath = mode_filename)\r\n",
        "\r\n",
        "    # Store repetition number\r\n",
        "    config[\"repeat\"] = repeat\r\n",
        "\r\n",
        "    # Evaluate model and store results \r\n",
        "    for (group_sample, label) in eval_samples:\r\n",
        "      # Evaluate performance\r\n",
        "      total_squared_distance, avg_distance, avg_prediction, avg_actual = evaluate_performance(group_sample, model)\r\n",
        "      # Store results      \r\n",
        "      config[\"eval_\" + label + \"_sample_avg\"] = avg_actual\r\n",
        "      config[\"eval_\" + label + \"_prediction_avg\"] = avg_prediction\r\n",
        "      config[\"eval_\" + label + \"_avg_error\"] = avg_distance\r\n",
        "      config[\"eval_\" + label + \"_total_squared_error\"] = total_squared_distance\r\n",
        "\r\n",
        "    # Write everything to results\r\n",
        "    config[\"model_iteration\"] = model_name\r\n",
        "    results.append(config)\r\n",
        "\r\n",
        "    # Free memory\r\n",
        "    del train_dataset   \r\n",
        "    del model\r\n",
        "    \r\n",
        "    # Do garbage collection explicitly \r\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iHoz2uZ2zYd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9odUjj16ZFf_"
      },
      "source": [
        "# Data Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snY549h-b8E7"
      },
      "source": [
        "# Create dataframe from results\r\n",
        "result_df = pd.DataFrame(results)\r\n",
        "# Write dataframe to excel\r\n",
        "result_df.to_excel(run_dir + \"/results.xlsx\")\r\n",
        "# Copy config json to run directory\r\n",
        "!cp -b /content/twrds_unbiased_anns/configs/$config_filename $run_dir/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0N_7ERsFZhW"
      },
      "source": [
        "# Set target zip name\r\n",
        "run_zip = run_name + \".zip\"\r\n",
        "\r\n",
        "# Zip run folder\r\n",
        "!zip -r /content/$run_zip $run_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIGBTJ2T0rXw"
      },
      "source": [
        "# Download run zip\r\n",
        "from google.colab import files\r\n",
        "files.download(\"/content/{}\".format(run_zip))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}